{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3c5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./models')\n",
    "\n",
    "import pywt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from models import GridFlow_lstm_mlp as  GridFlow\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_utils.tools import EarlyStopping, adjust_learning_rate, visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bed49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize_series(series, eps=1e-8):\n",
    "    mean = np.mean(series)\n",
    "    std = np.std(series)\n",
    "    standardized_series = (series - mean) / (std + eps)\n",
    "    return standardized_series, mean, std\n",
    "\n",
    "def unscale_predictions(predictions, mean, std, eps=1e-8):\n",
    "    return predictions * (std+eps) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccff7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_series(series, method_decom, period=24, wavelet='db4', level=5):\n",
    "    \"\"\"\n",
    "    Decomposes a time series into trend and seasonal+residual components.\n",
    "    Assumes hourly data by default (period=24).\n",
    "    \"\"\"\n",
    "    if method_decom == 'seasonal_decompose':\n",
    "       result = seasonal_decompose(series, model='additive', period=period, extrapolate_trend='freq')\n",
    "       trend = result.trend\n",
    "       seasonal_plus_resid = series - trend\n",
    "\n",
    "       # Handle NaNs from the trend's boundary effects\n",
    "       # trend = pd.Series(trend).fillna(method='bfill').fillna(method='ffill').values\n",
    "       trend = pd.Series(trend).bfill().ffill().values\n",
    "       seasonal_plus_resid = pd.Series(seasonal_plus_resid).fillna(0).values\n",
    "\n",
    "       return trend, seasonal_plus_resid\n",
    "    \n",
    "   \n",
    "    ##Decomposes a time series into trend and seasonal+residual components using wavelet transform, adjust level to get more in depth decompostion.\n",
    "\n",
    "    elif method_decom == 'wavelet':\n",
    "        if level is None:\n",
    "            level = pywt.dwt_max_level(len(series), pywt.Wavelet(wavelet).dec_len)\n",
    "\n",
    "        coeffs = pywt.wavedec(series, wavelet, level=level)\n",
    "\n",
    "        # Keep only the approximation, set detail coeffs to zero for clean trend\n",
    "        trend_coeffs = [coeffs[0]] + [np.zeros_like(c) for c in coeffs[1:]]\n",
    "        trend = pywt.waverec(trend_coeffs, wavelet)[:len(series)]\n",
    "\n",
    "        seasonal_plus_resid = series - trend\n",
    "        seasonal_plus_resid = pd.Series(seasonal_plus_resid).fillna(0).values\n",
    "\n",
    "        return trend, seasonal_plus_resid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95461cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposedTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, backcast_length, forecast_length, method_decom, stride=1, period=24):\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stride = stride\n",
    "        self.method_decom = method_decom\n",
    "        # Decompose the series into trend and seasonality+residual\n",
    "        trend, seasonality = decompose_series(series, method_decom, period=period)\n",
    "\n",
    "        # Standardize each component\n",
    "        self.trend, self.trend_mean, self.trend_std = standardize_series(trend)\n",
    "        self.season, self.season_mean, self.season_std = standardize_series(seasonality)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.trend) - self.backcast_length - self.forecast_length) // self.stride + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.stride\n",
    "\n",
    "        # Inputs\n",
    "        trend_input = self.trend[start : start + self.backcast_length]\n",
    "        season_input = self.season[start : start + self.backcast_length]\n",
    "\n",
    "        # Targets\n",
    "        trend_target = self.trend[start + self.backcast_length : start + self.backcast_length + self.forecast_length]\n",
    "        season_target = self.season[start + self.backcast_length : start + self.backcast_length + self.forecast_length]\n",
    "\n",
    "        return {\n",
    "            'trend_input': torch.tensor(trend_input, dtype=torch.float32),\n",
    "            'season_input': torch.tensor(season_input, dtype=torch.float32),\n",
    "            'trend_target': torch.tensor(trend_target, dtype=torch.float32),\n",
    "            'season_target': torch.tensor(season_target, dtype=torch.float32),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8632d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_time_split(series, train_len=4320, val_len=2160):\n",
    "    total_len = len(series)\n",
    "\n",
    "    if total_len < train_len + val_len + 1:  # not enough for all three splits\n",
    "        return None, None, None\n",
    "    \n",
    "    train_data = series[:train_len]\n",
    "    val_data = series[train_len:train_len+val_len]\n",
    "    test_data = series[train_len:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f97fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(folder_path, backcast_length, forecast_length, method_decom, stride=1, period=24):\n",
    "   \n",
    "    train_datasets = []\n",
    "    val_datasets = []\n",
    "    test_datasets = []\n",
    "\n",
    "    for region in os.listdir(folder_path):\n",
    "        region_path = os.path.join(folder_path, region)\n",
    "\n",
    "        for building in os.listdir(region_path):\n",
    "\n",
    "            if building.endswith('.csv'):\n",
    "                file_path = os.path.join(region_path, building)\n",
    "                df = pd.read_csv(file_path)\n",
    "                energy_data = df['energy'].values\n",
    "                train_data, val_data, test_data = fixed_time_split(energy_data)\n",
    "\n",
    "                if train_data is None:\n",
    "                    continue\n",
    "\n",
    "                train_dataset = DecomposedTimeSeriesDataset(train_data, backcast_length, forecast_length, method_decom,stride,period)\n",
    "                val_dataset = DecomposedTimeSeriesDataset(val_data, backcast_length, forecast_length,method_decom, stride,period)\n",
    "                test_dataset = DecomposedTimeSeriesDataset(test_data, backcast_length, forecast_length,method_decom, stride,period)\n",
    "             \n",
    "                train_datasets.append(train_dataset)\n",
    "                val_datasets.append(val_dataset)\n",
    "                test_datasets.append(test_dataset)\n",
    "                \n",
    "\n",
    "\n",
    "            elif building.endswith('.parquet'):\n",
    "                file_path = os.path.join(region_path, building)\n",
    "                df = pd.read_parquet(file_path)\n",
    "\n",
    "                if 'energy' not in df.columns:\n",
    "                    continue  # Skip if energy column is missing\n",
    "\n",
    "                energy_data = df['energy'].values\n",
    "                train_data, val_data, test_data = fixed_time_split(energy_data)\n",
    "\n",
    "                if train_data is None:\n",
    "                    continue\n",
    "\n",
    "                # Create TimeSeriesDataset for each split\n",
    "                train_dataset = DecomposedTimeSeriesDataset(train_data, backcast_length, forecast_length, method_decom,stride,period)\n",
    "                val_dataset = DecomposedTimeSeriesDataset(val_data, backcast_length, forecast_length,method_decom, stride,period)\n",
    "                test_dataset = DecomposedTimeSeriesDataset(test_data, backcast_length, forecast_length, method_decom,stride,period)\n",
    "             \n",
    "                train_datasets.append(train_dataset)\n",
    "                val_datasets.append(val_dataset)\n",
    "                test_datasets.append(test_dataset)\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Wrong file format!\")\n",
    "\n",
    "    if len(train_datasets) == 0:\n",
    "        raise RuntimeError(\"No valid parquet datasets found.\")\n",
    "    \n",
    "    print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "    # Combine all datasets for each split\n",
    "    combined_train_dataset = ConcatDataset(train_datasets)\n",
    "    combined_val_dataset = ConcatDataset(val_datasets)\n",
    "    combined_test_dataset = ConcatDataset(test_datasets)\n",
    "    print(len(combined_train_dataset), len(combined_val_dataset), len(combined_test_dataset))\n",
    "\n",
    "    return combined_train_dataset, combined_val_dataset, combined_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee461fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, criterion, optimizer, device, train_loader, val_loader, param):\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = args['patience']\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    early_stop = False\n",
    "\n",
    "    num_epochs = args[\"num_epochs\"]\n",
    "    train_start_time = time()  # Start timer \n",
    "\n",
    "    t_loss = []\n",
    "    v_loss = []\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break  \n",
    "\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        epoch_start_time = time()  # Start epoch timer\n",
    "\n",
    "        # Progress bar for the training loop\n",
    "        with tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, batch in enumerate(pbar):\n",
    "                trend_input = batch['trend_input'].to(device)\n",
    "                season_input = batch['season_input'].to(device)\n",
    "                trend_target = batch['trend_target'].to(device)\n",
    "                season_target = batch['season_target'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass: Get trend and season predictions\n",
    "                trend_pred, season_pred = model(trend_input, season_input)\n",
    "\n",
    "                # Calculate loss for trend and season separately (you could also add weightings)\n",
    "                loss_trend = criterion(trend_pred, trend_target)\n",
    "                loss_season = criterion(season_pred, season_target)\n",
    "                \n",
    "                # Total loss is the sum of trend and season losses\n",
    "                # total_loss = 0.3 * loss_trend + 0.7 * loss_season\n",
    "\n",
    "                sum_loss = loss_trend + loss_season\n",
    "                alpha = loss_season / sum_loss\n",
    "                beta = loss_trend / sum_loss\n",
    "\n",
    "                total_loss = alpha * loss_trend + beta * loss_season\n",
    "\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses.append(total_loss.item())\n",
    "\n",
    "                if i % 5 ==0:\n",
    "                    pbar.set_postfix(loss=total_loss.item(), elapsed=f\"{time() - epoch_start_time:.2f}s\")\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        t_loss.append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_true_val = []\n",
    "        y_pred_val = []\n",
    "\n",
    "        # Progress bar for the validation loop\n",
    "        with tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for batch in pbar:\n",
    "                trend_input = batch['trend_input'].to(device)\n",
    "                season_input = batch['season_input'].to(device)\n",
    "                trend_target = batch['trend_target'].to(device)\n",
    "                season_target = batch['season_target'].to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    trend_pred, season_pred = model(trend_input, season_input)\n",
    "                    loss_trend = criterion(trend_pred, trend_target)\n",
    "                    loss_season = criterion(season_pred, season_target)\n",
    "                    # val_loss = 0.3 * loss_trend + 0.7 * loss_season\n",
    "\n",
    "\n",
    "                    sum_loss = loss_trend + loss_season\n",
    "                    alpha = loss_season / sum_loss\n",
    "                    beta = loss_trend / sum_loss\n",
    "\n",
    "                    val_loss = alpha * loss_trend + beta * loss_season\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    # Collect true and predicted values for RMSE calculation\n",
    "                    y_true_val.extend(trend_target.cpu().numpy())\n",
    "                    y_pred_val.extend(trend_pred.cpu().numpy())\n",
    "                    y_true_val.extend(season_target.cpu().numpy())\n",
    "                    y_pred_val.extend(season_pred.cpu().numpy())\n",
    "\n",
    "        # Calculate average validation loss and RMSE\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        v_loss.append(avg_val_loss)\n",
    "\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, RMSE: {rmse_val:.4f}')\n",
    "\n",
    "        # Save the best model parameters\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            os.makedirs(args[\"finetuned_model_save_path\"], exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{args[\"finetuned_model_save_path\"]}/best_model.pth')\n",
    "          \n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                early_stop = True\n",
    "\n",
    "        # Adjust learning rate\n",
    "        adjust_learning_rate(optimizer, epoch + 1, args)\n",
    "\n",
    "\n",
    "    total_training_time = time() - train_start_time\n",
    "    print(f'Total Training Time: {total_training_time:.2f}s')\n",
    "\n",
    "    # Save loss data\n",
    "    loss_data = {\n",
    "        \"param\": param,\n",
    "        \"train_loss\": t_loss,\n",
    "        \"val_loss\": v_loss\n",
    "    }\n",
    "\n",
    "    loss_data_path = f'{args[\"finetuned_model_save_path\"]}/loss_data.json'\n",
    "    with open(loss_data_path, \"w\") as f:\n",
    "        json.dump(loss_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d063bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 83 1418\n",
      "170578 81838 1638936\n",
      "Model's parameter count is: 131351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.0048, Val Loss: 0.0130, RMSE: 0.3929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 0.0045, Val Loss: 0.0100, RMSE: 0.3896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 0.0041, Val Loss: 0.0103, RMSE: 0.3905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Train Loss: 0.0038, Val Loss: 0.0090, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Train Loss: 0.0037, Val Loss: 0.0091, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Train Loss: 0.0036, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Train Loss: 0.0036, Val Loss: 0.0086, RMSE: 0.3880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Train Loss: 0.0036, Val Loss: 0.0088, RMSE: 0.3880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.0035, Val Loss: 0.0089, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Train Loss: 0.0035, Val Loss: 0.0088, RMSE: 0.3881\n",
      "Early stopping at epoch 18\n",
      "Total Training Time: 132.21s\n"
     ]
    }
   ],
   "source": [
    "config_file = \"./configs/gridflow_lstm_mlp_wavelet_finetune.json\"\n",
    "with open(config_file, 'r') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "train_datasets, val_datasets,_ = load_datasets(args['test_dataset_path'], args['backcast_length'], args['forecast_length'],args['method_decom'], args['stride'])\n",
    "\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_datasets, batch_size=args['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_datasets, batch_size=args['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# check device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define N-BEATS model\n",
    "model = GridFlow.Model(\n",
    "    device=device,\n",
    "    num_blocks_per_stack=args['num_blocks_per_stack'],\n",
    "    forecast_length=args['forecast_length'],\n",
    "    backcast_length=args['backcast_length'],\n",
    "    patch_size=args['patch_size'],\n",
    "    num_patches=args['backcast_length'] // args['patch_size'],\n",
    "    thetas_dim=args['thetas_dim'],\n",
    "    hidden_dim=args['hidden_dim'],\n",
    "    embed_dim=args['embed_dim'],\n",
    "    num_heads=args['num_heads'],\n",
    "    ff_hidden_dim=args['ff_hidden_dim'],\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f'{args[\"pretrained_model_path\"]}/best_model.pth'))\n",
    "# model's parameters\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Model's parameter count is:\", param)\n",
    "\n",
    "# Define loss and optimizer\n",
    "if args['loss'] == 'mse':\n",
    "    criterion = torch.nn.MSELoss()\n",
    "else:\n",
    "    criterion = torch.nn.HuberLoss(reduction=\"mean\", delta=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args[\"learning_rate\"])\n",
    "\n",
    "# training the model and save best parameters\n",
    "train(args, model, criterion, optimizer, device, train_loader, val_loader, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c40a1",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67def31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./model')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models import GridFlow_lstm_mlp as GridFlow\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from my_utils.metrics import cal_cvrmse, cal_mae, cal_mse, cal_nrmse\n",
    "from my_utils.decompose_normalize import standardize_series, unscale_predictions, decompose_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ec9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, criterion, device):\n",
    "\n",
    "    folder_path = args['test_dataset_path']\n",
    "    result_path = args['result_path']\n",
    "    backcast_length = args['backcast_length']\n",
    "    forecast_length = args['forecast_length']\n",
    "    stride = args['stride']\n",
    "    period = 24\n",
    "    method_decom = args['method_decom']\n",
    "\n",
    "\n",
    "    median_res = []  \n",
    "    for region in os.listdir(folder_path):\n",
    "\n",
    "        region_path = os.path.join(folder_path, region)\n",
    "\n",
    "        results_path = os.path.join(result_path, region)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "        res = []\n",
    "\n",
    "        for building in os.listdir(region_path):\n",
    "\n",
    "            \n",
    "\n",
    "            if building.endswith('.csv') or building.endswith('.parquet'):\n",
    "                file_path = os.path.join(region_path, building)\n",
    "                if building.endswith('.csv'):\n",
    "                    building_id = building.rsplit(\".csv\",1)[0]\n",
    "                    df = pd.read_csv(file_path)\n",
    "                else:\n",
    "                    building_id = building.rsplit(\".parquet\",1)[0]\n",
    "                    df = pd.read_parquet(file_path)\n",
    "                energy_data = df['energy'].values\n",
    "                train_data, val_data, test_data = fixed_time_split(energy_data)\n",
    "\n",
    "                if train_data is None:\n",
    "                    continue\n",
    "                dataset = DecomposedTimeSeriesDataset(test_data, backcast_length, forecast_length, method_decom, stride, period)\n",
    "                \n",
    "                # test phase\n",
    "                model.eval()\n",
    "                test_losses = []\n",
    "                y_true_trend = []\n",
    "                y_true_seasonal = []\n",
    "                y_pred_trend = []\n",
    "                y_pred_seasonal = []\n",
    "\n",
    "                # test loop\n",
    "                for batch in tqdm(DataLoader(dataset, batch_size=1, num_workers=4), desc=f\"Testing {building_id}\", leave=False):\n",
    "                    trend_input = batch['trend_input'].to(device)\n",
    "                    season_input = batch['season_input'].to(device)\n",
    "                    trend_target = batch['trend_target'].to(device)\n",
    "                    season_target = batch['season_target'].to(device)\n",
    "                    with torch.no_grad():\n",
    "                        trend_pred, season_pred = model(trend_input, season_input)\n",
    "                        loss_trend = criterion(trend_pred, trend_target)\n",
    "                        loss_season = criterion(season_pred, season_target)\n",
    "\n",
    "                        sum_loss = loss_trend + loss_season\n",
    "                        alpha = loss_season / sum_loss\n",
    "                        beta = loss_trend / sum_loss\n",
    "\n",
    "                        loss = alpha * loss_trend + beta * loss_season\n",
    "                        test_losses.append(loss.item())\n",
    "                        \n",
    "                        # Collect true and predicted values for RMSE calculation\n",
    "                        y_true_trend.extend(trend_target.cpu().numpy())\n",
    "                        y_true_seasonal.extend(season_target.cpu().numpy())\n",
    "                        y_pred_trend.extend(trend_pred.cpu().numpy())\n",
    "                        y_pred_seasonal.extend(season_pred.cpu().numpy())\n",
    "                        \n",
    "                # Calculate average validation loss and RMSE\n",
    "                y_true_combine_trend = np.concatenate(y_true_trend, axis=0)\n",
    "                y_true_combine_seasonal = np.concatenate(y_true_seasonal, axis=0)\n",
    "                y_pred_combine_trend = np.concatenate(y_pred_trend, axis=0)\n",
    "                y_pred_combine_seasonal = np.concatenate(y_pred_seasonal, axis=0)\n",
    "                avg_test_loss = np.mean(test_losses)\n",
    "\n",
    "                y_pred_combine = y_pred_combine_seasonal + y_pred_combine_trend\n",
    "                y_true_combine = y_true_combine_seasonal + y_true_combine_trend\n",
    "                \n",
    "                y_true_combine_trend_unscaled = unscale_predictions(y_true_combine_trend, dataset.trend_mean, dataset.trend_std)\n",
    "                y_pred_combine_trend_unscaled = unscale_predictions(y_pred_combine_trend, dataset.trend_mean, dataset.trend_std)\n",
    "                y_true_combine_seasonal_unscaled = unscale_predictions(y_true_combine_seasonal, dataset.season_mean, dataset.season_std)\n",
    "                y_pred_combine_seasonal_unscaled = unscale_predictions(y_pred_combine_seasonal, dataset.season_mean, dataset.season_std)\n",
    "\n",
    "                y_pred_combine_unscaled = y_pred_combine_seasonal_unscaled + y_pred_combine_trend_unscaled\n",
    "                y_true_combine_unscaled = y_true_combine_seasonal_unscaled + y_true_combine_trend_unscaled\n",
    "\n",
    "                \n",
    "                # Calculate CVRMSE, NRMSE, MAE on unscaled data\n",
    "                cvrmse = cal_cvrmse(y_pred_combine_unscaled, y_true_combine_unscaled)\n",
    "                nrmse = cal_nrmse(y_pred_combine_unscaled, y_true_combine_unscaled)\n",
    "                mae = cal_mae(y_pred_combine_unscaled, y_true_combine_unscaled)\n",
    "                mse = cal_mse(y_pred_combine_unscaled, y_true_combine_unscaled)\n",
    "                mae_norm = cal_mae(y_pred_combine, y_true_combine)\n",
    "                mse_norm = cal_mse(y_pred_combine, y_true_combine)\n",
    "\n",
    "                res.append([building_id, cvrmse, nrmse, mae, mae_norm, mse, mse_norm, avg_test_loss])\n",
    "\n",
    "        columns = ['building_ID', 'CVRMSE', 'NRMSE', 'MAE', 'MAE_NORM', 'MSE', 'MSE_NORM', 'Avg_Test_Loss']\n",
    "        df = pd.DataFrame(res, columns=columns)\n",
    "        df.to_csv(\"{}/{}.csv\".format(results_path, 'result'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "        med_nrmse = df['NRMSE'].median()\n",
    "        med_mae = df['MAE'].median()\n",
    "        med_mae_norm = df['MAE_NORM'].median()\n",
    "        med_mse = df['MSE'].median()\n",
    "        med_mse_norm = df['MSE_NORM'].median()\n",
    "\n",
    "        median_res.append([region, med_nrmse, med_mae, med_mae_norm, med_mse, med_mse_norm])\n",
    "\n",
    "    med_columns = ['Dataset','NRMSE', 'MAE', 'MAE_NORM', 'MSE', 'MSE_NORM']\n",
    "    median_df = pd.DataFrame(median_res, columns=med_columns)\n",
    "    median_df.to_csv(\"{}/{}.csv\".format(result_path, 'median_results_of_buildings'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "410fd126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  \n",
    "    config_file = \"./configs/gridflow_lstm_mlp_wavelet_finetune.json\"\n",
    "    with open(config_file, 'r') as f:\n",
    "        args = json.load(f)\n",
    "\n",
    "    # check device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Define GridFlow model\n",
    "    model = GridFlow.Model(\n",
    "        device=device,\n",
    "        num_blocks_per_stack=args['num_blocks_per_stack'],\n",
    "        forecast_length=args['forecast_length'],\n",
    "        backcast_length=args['backcast_length'],\n",
    "        patch_size=args['patch_size'],\n",
    "        num_patches=args['backcast_length'] // args['patch_size'],\n",
    "        thetas_dim=args['thetas_dim'],\n",
    "        hidden_dim=args['hidden_dim'],\n",
    "        embed_dim=args['embed_dim'],\n",
    "        num_heads=args['num_heads'],\n",
    "        ff_hidden_dim=args['ff_hidden_dim'],\n",
    "    ).to(device)\n",
    "\n",
    "    model_load_path = '{}/best_model.pth'.format(args['finetuned_model_save_path'])\n",
    "    model.load_state_dict(torch.load(model_load_path, weights_only=True))\n",
    "\n",
    "\n",
    "\n",
    "    # Define loss\n",
    "    if args['loss'] == 'mse':\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    else:\n",
    "        criterion = torch.nn.HuberLoss(reduction=\"mean\", delta=1)\n",
    "\n",
    "\n",
    "    # training the model and save best parameters\n",
    "    test(args, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf00063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shivam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
